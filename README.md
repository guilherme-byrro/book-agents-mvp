# ğŸ“š Book Agents MVP

Sistema de agentes de IA para geraÃ§Ã£o de conteÃºdo narrativo usando Ollama e Llama 3.1.

## âœ¨ CaracterÃ­sticas

- ğŸ¤– **Agentes Inteligentes**: Writer, Planner e Editor com IA
- ğŸ”„ **Sistema HÃ­brido**: IA quando disponÃ­vel, fallback rule-based sempre funcional
- ğŸŒ **Interface Web**: Interface Gradio moderna e intuitiva
- ğŸ“‹ **Planos DinÃ¢micos**: AdaptaÃ§Ã£o automÃ¡tica ao brief fornecido
- ğŸ› ï¸ **DiagnÃ³stico Integrado**: Script para troubleshooting do Ollama
- âš¡ **RecuperaÃ§Ã£o Robusta**: Tratamento inteligente de timeouts e erros

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

1. **Python 3.8+**
2. **Ollama** instalado e rodando
3. **Modelo Llama 3.1** (ou compatÃ­vel)

### Setup

1. **Clone o repositÃ³rio:**
   ```bash
   git clone https://github.com/guilherme-byrro/book-agents-mvp.git
   cd book-agents-mvp
   ```

2. **Crie ambiente virtual:**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # Windows
   # source venv/bin/activate  # Linux/Mac
   ```

3. **Instale dependÃªncias:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure Ollama:**
   ```bash
   # Instalar modelo
   ollama pull llama3.1
   
   # Iniciar serviÃ§o
   ollama serve
   ```

## ğŸ¯ Uso

### Interface Web (Recomendado)

```bash
python gradio_app.py
```

Acesse: http://localhost:7860

### Linha de Comando

```bash
python app.py --brief "Escreva um encontro tenso no MASP entre Ivana e Dr. Manoel, noite chuvosa."
```

### DiagnÃ³stico

```bash
python diagnose_ollama.py
```

## ğŸ“ Estrutura do Projeto

```
book-agents-mvp/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ writer.py      # Agente escritor com IA
â”‚   â”œâ”€â”€ planner.py     # Agente planejador
â”‚   â””â”€â”€ editor.py      # Agente editor
â”œâ”€â”€ engine/
â”‚   â””â”€â”€ llm.py         # Interface com Ollama
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ style_guide.md # Guia de estilo
â”‚   â””â”€â”€ canon/         # Dados do projeto
â”œâ”€â”€ output/            # Cenas geradas
â”œâ”€â”€ app.py            # Interface linha de comando
â”œâ”€â”€ gradio_app.py     # Interface web
â”œâ”€â”€ diagnose_ollama.py # Script de diagnÃ³stico
â”œâ”€â”€ config.yaml       # ConfiguraÃ§Ã£o do modelo
â””â”€â”€ requirements.txt  # DependÃªncias
```

## âš™ï¸ ConfiguraÃ§Ã£o

### config.yaml
```yaml
provider:
  name: ollama
  model: llama3.1  # ou llama3.1:8b para modelo menor
```

### Modelos Recomendados

- **llama3.1** - Qualidade mÃ¡xima (requer mais RAM)
- **llama3.1:8b** - Equilibrio qualidade/performance
- **llama3.1:7b** - Mais rÃ¡pido, menos RAM

## ğŸ”§ Troubleshooting

### Ollama nÃ£o conecta
```bash
# Verificar se estÃ¡ rodando
ollama list

# Iniciar serviÃ§o
ollama serve
```

### Timeout na geraÃ§Ã£o
- Use modelo menor: `ollama pull llama3.1:8b`
- Feche outros programas pesados
- Aumente RAM disponÃ­vel

### Modelo nÃ£o encontrado
```bash
ollama pull llama3.1
```

## ğŸ¨ Exemplos de Briefs

- "Escreva um encontro tenso no MASP entre Ivana e Dr. Manoel, noite chuvosa."
- "Crie uma cena de suspense em uma biblioteca antiga, com dois personagens discutindo um segredo."
- "Descreva um diÃ¡logo emotivo entre pai e filha em um cafÃ© movimentado de SÃ£o Paulo."
- "Escreva uma cena de aÃ§Ã£o em um metrÃ´ lotado durante o horÃ¡rio de rush."

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Commit: `git commit -m 'Adiciona nova funcionalidade'`
4. Push: `git push origin feature/nova-funcionalidade`
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

MIT License - veja [LICENSE](LICENSE) para detalhes.

## ğŸ”— Links Ãšteis

- [Ollama](https://ollama.ai/) - Plataforma de modelos locais
- [Gradio](https://gradio.app/) - Interface web para ML
- [Llama 3.1](https://llama.meta.com/) - Modelo de linguagem da Meta

---

**Desenvolvido com â¤ï¸ usando IA e Python**
