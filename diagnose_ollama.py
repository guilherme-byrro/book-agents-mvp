#!/usr/bin/env python3
"""
Ollama Diagnostic Script - Check Ollama status and troubleshoot issues
"""

import requests
import json
import yaml
import time
from pathlib import Path

def load_config():
    """Load configuration"""
    config_path = Path(__file__).parent / "config.yaml"
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"‚ùå Erro ao carregar config.yaml: {e}")
        return None

def check_ollama_service():
    """Check if Ollama service is running"""
    print("üîç Verificando se Ollama est√° rodando...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama est√° rodando!")
            return True
        else:
            print(f"‚ùå Ollama respondeu com status {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("‚ùå N√£o foi poss√≠vel conectar ao Ollama na porta 11434")
        print("üí° Certifique-se de que o Ollama est√° rodando: ollama serve")
        return False
    except Exception as e:
        print(f"‚ùå Erro ao conectar: {e}")
        return False

def list_available_models():
    """List available models"""
    print("\nüìã Verificando modelos dispon√≠veis...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            models = data.get('models', [])
            
            if models:
                print(f"‚úÖ {len(models)} modelo(s) encontrado(s):")
                for model in models:
                    name = model.get('name', 'N/A')
                    size = model.get('size', 0)
                    size_mb = size / (1024 * 1024) if size else 0
                    print(f"   - {name} ({size_mb:.1f} MB)")
                return [m.get('name', '') for m in models]
            else:
                print("‚ö†Ô∏è Nenhum modelo encontrado!")
                print("üí° Instale um modelo: ollama pull llama3.1")
                return []
        else:
            print(f"‚ùå Erro ao listar modelos: {response.status_code}")
            return []
    except Exception as e:
        print(f"‚ùå Erro ao listar modelos: {e}")
        return []

def check_configured_model(config):
    """Check if configured model is available"""
    if not config:
        return False
    
    configured_model = config.get('provider', {}).get('model', 'llama3.1')
    print(f"\nüéØ Verificando modelo configurado: {configured_model}")
    
    available_models = list_available_models()
    
    # Check if configured model is in available models
    model_found = any(configured_model in model for model in available_models)
    
    if model_found:
        print(f"‚úÖ Modelo {configured_model} est√° dispon√≠vel!")
        return True
    else:
        print(f"‚ùå Modelo {configured_model} n√£o encontrado!")
        print(f"üí° Instale o modelo: ollama pull {configured_model}")
        return False

def test_model_generation(model_name):
    """Test model generation with a simple prompt"""
    print(f"\nüß™ Testando gera√ß√£o com {model_name}...")
    
    test_payload = {
        "model": model_name,
        "prompt": "Escreva uma frase simples sobre o tempo.",
        "stream": False,
        "options": {
            "num_predict": 50,
            "temperature": 0.7
        }
    }
    
    try:
        start_time = time.time()
        response = requests.post(
            "http://localhost:11434/api/generate",
            json=test_payload,
            timeout=60
        )
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            generated_text = result.get('response', '').strip()
            duration = end_time - start_time
            
            print(f"‚úÖ Teste bem-sucedido! ({duration:.1f}s)")
            print(f"üìù Resposta: {generated_text[:100]}...")
            return True
        else:
            print(f"‚ùå Erro na gera√ß√£o: {response.status_code}")
            print(f"üìÑ Resposta: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print("‚è∞ Timeout! O modelo pode estar sobrecarregado ou muito lento.")
        print("üí° Tente um modelo menor ou aumente o timeout.")
        return False
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
        return False

def provide_recommendations():
    """Provide troubleshooting recommendations"""
    print("\nüîß RECOMENDA√á√ïES DE TROUBLESHOOTING:")
    print()
    print("1. **Iniciar Ollama:**")
    print("   ollama serve")
    print()
    print("2. **Instalar modelo (se necess√°rio):**")
    print("   ollama pull llama3.1")
    print("   ollama pull llama3.1:8b  # vers√£o menor")
    print()
    print("3. **Verificar modelos instalados:**")
    print("   ollama list")
    print()
    print("4. **Testar modelo manualmente:**")
    print("   ollama run llama3.1")
    print()
    print("5. **Se o modelo for muito lento:**")
    print("   - Use um modelo menor (llama3.1:8b)")
    print("   - Aumente a RAM dispon√≠vel")
    print("   - Feche outros programas pesados")
    print()
    print("6. **Verificar logs do Ollama:**")
    print("   - Windows: %USERPROFILE%\\.ollama\\logs")
    print("   - Linux/Mac: ~/.ollama/logs")

def main():
    """Main diagnostic function"""
    print("üöÄ DIAGN√ìSTICO DO OLLAMA")
    print("=" * 50)
    
    # Load config
    config = load_config()
    if config:
        model_name = config.get('provider', {}).get('model', 'llama3.1')
        print(f"üìã Modelo configurado: {model_name}")
    else:
        model_name = 'llama3.1'
        print("‚ö†Ô∏è Usando modelo padr√£o: llama3.1")
    
    # Check Ollama service
    if not check_ollama_service():
        provide_recommendations()
        return
    
    # List models
    available_models = list_available_models()
    
    # Check configured model
    if not check_configured_model(config):
        provide_recommendations()
        return
    
    # Test generation
    if not test_model_generation(model_name):
        provide_recommendations()
        return
    
    print("\nüéâ DIAGN√ìSTICO CONCLU√çDO!")
    print("‚úÖ Ollama est√° funcionando corretamente!")
    print("üöÄ Voc√™ pode usar os agentes com IA agora!")

if __name__ == "__main__":
    main()
